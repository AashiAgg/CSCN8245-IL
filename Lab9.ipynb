{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Assignment 9\n",
    "#### Student Name:AASHI AGGARWAL\n",
    "#### ID: 8920299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the sample sentences above. You are required for this assignment to implement four functions **from scratch**. <br>\n",
    "You are required to preprocess the text and apply the tokenization process as done in assignment 8. (3)\n",
    "***THEN***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'versatile', 'programming', 'language', 'JavaScript', 'widely', 'used', 'web', 'development', 'Java', 'known', 'platform', 'independence', 'Programming', 'involves', 'writing', 'code', 'solve', 'problems', 'Data', 'structures', 'crucial', 'efficient', 'programming', 'Algorithms', 'step-by-step', 'instructions', 'solving', 'problems', 'Version', 'control', 'systems', 'help', 'manage', 'code', 'changes', 'collaboration', 'Debugging', 'process', 'finding', 'fixing', 'errors', 'code', 'Web', 'frameworks', 'simplify', 'development', 'web', 'applications', 'Artificial', 'intelligence', 'applied', 'various', 'programming', 'tasks']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Andy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Sample sentences\n",
    "sample_sentences = [\n",
    "    \"Python is a versatile programming language.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize and clean the sentences\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "cleaned_words = []\n",
    "for sentence in sample_sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    cleaned_words.extend([word for word in words if word.lower() not in stop_words and word not in string.punctuation])\n",
    "\n",
    "# Print the cleaned words\n",
    "print(cleaned_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 1: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the inverted index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: [1]\n",
      "versatil: [2]\n",
      "program: [3, 14, 24, 54]\n",
      "languag: [4]\n",
      "javascript: [5]\n",
      "wide: [6]\n",
      "use: [7]\n",
      "web: [8, 44, 48]\n",
      "develop: [9, 47]\n",
      "java: [10]\n",
      "known: [11]\n",
      "platform: [12]\n",
      "independ: [13]\n",
      "involv: [15]\n",
      "write: [16]\n",
      "code: [17, 35, 43]\n",
      "solv: [18, 28]\n",
      "problem: [19, 29]\n",
      "data: [20]\n",
      "structur: [21]\n",
      "crucial: [22]\n",
      "effici: [23]\n",
      "algorithm: [25]\n",
      "stepbystep: [26]\n",
      "instruct: [27]\n",
      "version: [30]\n",
      "control: [31]\n",
      "system: [32]\n",
      "help: [33]\n",
      "manag: [34]\n",
      "chang: [36]\n",
      "collabor: [37]\n",
      "debug: [38]\n",
      "process: [39]\n",
      "find: [40]\n",
      "fix: [41]\n",
      "error: [42]\n",
      "framework: [45]\n",
      "simplifi: [46]\n",
      "applic: [49]\n",
      "artifici: [50]\n",
      "intellig: [51]\n",
      "appli: [52]\n",
      "variou: [53]\n",
      "task: [55]\n"
     ]
    }
   ],
   "source": [
    "def get_inverted_index(list_of_sentence_tokens):\n",
    "    inverted_index = {}\n",
    "\n",
    "    for sentence_id, sentence in enumerate(list_of_sentence_tokens, start=1):\n",
    "        for word in set(sentence):  # Use set to get unique words in the sentence\n",
    "            inverted_index.setdefault(word, []).append(sentence_id)\n",
    "\n",
    "    return inverted_index\n",
    "\n",
    "# Apply preprocessing to each sentence\n",
    "cleanList = [preprocess_text(sentence) for sentence in cleaned_words]\n",
    "\n",
    "# Get the inverted index\n",
    "inverted_index = get_inverted_index(cleanList)\n",
    "\n",
    "# Print the inverted index\n",
    "for word, indices in inverted_index.items():\n",
    "    print(f\"{word}: {indices}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 2: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the Positional index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1, and the first token in the list is at position 0. Make sure to consider multiple appearance of the same token. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: {1: [0]}\n",
      "versatil: {2: [0]}\n",
      "program: {3: [0], 14: [0], 24: [0], 54: [0]}\n",
      "languag: {4: [0]}\n",
      "javascript: {5: [0]}\n",
      "wide: {6: [0]}\n",
      "use: {7: [0]}\n",
      "web: {8: [0], 44: [0], 48: [0]}\n",
      "develop: {9: [0], 47: [0]}\n",
      "java: {10: [0]}\n",
      "known: {11: [0]}\n",
      "platform: {12: [0]}\n",
      "independ: {13: [0]}\n",
      "involv: {15: [0]}\n",
      "write: {16: [0]}\n",
      "code: {17: [0], 35: [0], 43: [0]}\n",
      "solv: {18: [0], 28: [0]}\n",
      "problem: {19: [0], 29: [0]}\n",
      "data: {20: [0]}\n",
      "structur: {21: [0]}\n",
      "crucial: {22: [0]}\n",
      "effici: {23: [0]}\n",
      "algorithm: {25: [0]}\n",
      "stepbystep: {26: [0]}\n",
      "instruct: {27: [0]}\n",
      "version: {30: [0]}\n",
      "control: {31: [0]}\n",
      "system: {32: [0]}\n",
      "help: {33: [0]}\n",
      "manag: {34: [0]}\n",
      "chang: {36: [0]}\n",
      "collabor: {37: [0]}\n",
      "debug: {38: [0]}\n",
      "process: {39: [0]}\n",
      "find: {40: [0]}\n",
      "fix: {41: [0]}\n",
      "error: {42: [0]}\n",
      "framework: {45: [0]}\n",
      "simplifi: {46: [0]}\n",
      "applic: {49: [0]}\n",
      "artifici: {50: [0]}\n",
      "intellig: {51: [0]}\n",
      "appli: {52: [0]}\n",
      "variou: {53: [0]}\n",
      "task: {55: [0]}\n"
     ]
    }
   ],
   "source": [
    "def get_positional_index(list_of_sentence_tokens):\n",
    "    positional_index = {}\n",
    "\n",
    "    for sentence_id, sentence in enumerate(list_of_sentence_tokens, start=1):\n",
    "        for position, word in enumerate(sentence):\n",
    "            if word not in positional_index:\n",
    "                positional_index[word] = {sentence_id: [position]}\n",
    "            elif sentence_id not in positional_index[word]:\n",
    "                positional_index[word][sentence_id] = [position]\n",
    "            else:\n",
    "                positional_index[word][sentence_id].append(position)\n",
    "\n",
    "    return positional_index\n",
    "\n",
    "# Apply preprocessing to each sentence\n",
    "cleanList = [preprocess_text(sentence) for sentence in cleaned_words]\n",
    "\n",
    "# Get the positional index\n",
    "positional_index = get_positional_index(cleanList)\n",
    "\n",
    "# Print the positional index\n",
    "for word, positions in positional_index.items():\n",
    "    print(f\"{word}: {positions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 3: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the TF-IDF Matrix that is sufficient to represent the documents. Assume that each sentence is a document and the sentence ID starts from 1. (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'python': 4.007333185232471}, {'versatil': 4.007333185232471}, {'program': 2.6210388241125804}, {'languag': 4.007333185232471}, {'javascript': 4.007333185232471}, {'wide': 4.007333185232471}, {'use': 4.007333185232471}, {'web': 2.908720896564361}, {'develop': 3.3141860046725258}, {'java': 4.007333185232471}, {'known': 4.007333185232471}, {'platform': 4.007333185232471}, {'independ': 4.007333185232471}, {'program': 2.6210388241125804}, {'involv': 4.007333185232471}, {'write': 4.007333185232471}, {'code': 2.908720896564361}, {'solv': 3.3141860046725258}, {'problem': 3.3141860046725258}, {'data': 4.007333185232471}, {'structur': 4.007333185232471}, {'crucial': 4.007333185232471}, {'effici': 4.007333185232471}, {'program': 2.6210388241125804}, {'algorithm': 4.007333185232471}, {'stepbystep': 4.007333185232471}, {'instruct': 4.007333185232471}, {'solv': 3.3141860046725258}, {'problem': 3.3141860046725258}, {'version': 4.007333185232471}, {'control': 4.007333185232471}, {'system': 4.007333185232471}, {'help': 4.007333185232471}, {'manag': 4.007333185232471}, {'code': 2.908720896564361}, {'chang': 4.007333185232471}, {'collabor': 4.007333185232471}, {'debug': 4.007333185232471}, {'process': 4.007333185232471}, {'find': 4.007333185232471}, {'fix': 4.007333185232471}, {'error': 4.007333185232471}, {'code': 2.908720896564361}, {'web': 2.908720896564361}, {'framework': 4.007333185232471}, {'simplifi': 4.007333185232471}, {'develop': 3.3141860046725258}, {'web': 2.908720896564361}, {'applic': 4.007333185232471}, {'artifici': 4.007333185232471}, {'intellig': 4.007333185232471}, {'appli': 4.007333185232471}, {'variou': 4.007333185232471}, {'program': 2.6210388241125804}, {'task': 4.007333185232471}]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_TFIDF_matrix(list_of_sentence_tokens):\n",
    "    # Step 1: Create a set of unique words\n",
    "    unique_words = set(word for sentence in list_of_sentence_tokens for word in sentence)\n",
    "\n",
    "    # Step 2: Calculate IDF values for each unique word\n",
    "    num_documents = len(list_of_sentence_tokens)\n",
    "    idf_values = defaultdict(int)\n",
    "    for word in unique_words:\n",
    "        word_in_docs = sum(1 for sentence in list_of_sentence_tokens if word in sentence)\n",
    "        if word_in_docs > 0:\n",
    "            idf_values[word] = math.log(num_documents / word_in_docs)\n",
    "\n",
    "    # Step 3: Calculate TF-IDF matrix\n",
    "    tf_idf_matrix = []\n",
    "    for sentence in list_of_sentence_tokens:\n",
    "        sentence_tf_idf = {}\n",
    "        for word in sentence:\n",
    "            tf = sentence.count(word) / len(sentence)\n",
    "            idf = idf_values[word]\n",
    "            sentence_tf_idf[word] = tf * idf\n",
    "        tf_idf_matrix.append(sentence_tf_idf)\n",
    "\n",
    "    return tf_idf_matrix\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "result = get_TFIDF_matrix(cleanList)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 4: Create a method that takes as an input: (10)\n",
    " - a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences.\n",
    " - A method name: \"tfidf\", \"inverted\"\n",
    " - A Search Query\n",
    " - Return the rank of the sentences based on the given method and a query <br>\n",
    "\n",
    "***Hint: For inverted index we just want documents that have the query word/words, for tfidf you must show the ranking based on highest tfidf score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Documents (Inverted Index): [3, 14, 24, 54, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55]\n",
      "Ranked Documents (TF-IDF): [3, 14, 24, 54, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55]\n"
     ]
    }
   ],
   "source": [
    "def get_ranked_documents(list_of_sentence_tokens, method_name, search_query):\n",
    "    search_query_words = set(preprocess_text(search_query))  # Use set for faster lookup\n",
    "    rank_list = []\n",
    "\n",
    "    if method_name == \"inverted\":\n",
    "        inverted_index = get_inverted_index(list_of_sentence_tokens)\n",
    "        for i, sentence in enumerate(list_of_sentence_tokens, 1):\n",
    "            match = sum(1 for word in search_query_words if word in inverted_index and i in inverted_index[word])\n",
    "            rank_list.append((i, match))\n",
    "\n",
    "    elif method_name == \"tfidf\":\n",
    "        tf_idf_matrix = get_TFIDF_matrix(list_of_sentence_tokens)\n",
    "        for i, scores in enumerate(tf_idf_matrix, 1):\n",
    "            tfidf_score = sum(scores.get(word, 0) for word in search_query_words)\n",
    "            rank_list.append((i, tfidf_score))\n",
    "\n",
    "    rank_list.sort(key=lambda x: x[1], reverse=True)\n",
    "    ranked_documents = [index for index, _ in rank_list]\n",
    "\n",
    "    return ranked_documents\n",
    "\n",
    "# Apply preprocessing to each sentence\n",
    "cleanList = [preprocess_text(sentence) for sentence in cleaned_words]\n",
    "\n",
    "# Get the ranked documents\n",
    "inverted_ranked = get_ranked_documents(cleanList, 'inverted', 'programming')\n",
    "tfidf_ranked = get_ranked_documents(cleanList, 'tfidf', 'programming')\n",
    "\n",
    "\n",
    "print(\"Ranked Documents (Inverted Index):\", inverted_ranked)\n",
    "print(\"Ranked Documents (TF-IDF):\", tfidf_ranked)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
